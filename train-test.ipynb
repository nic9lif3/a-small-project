{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=pd.read_csv('testing.csv')\n",
    "a2=pd.read_csv('trainning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept1=[]\n",
    "accept2=[]\n",
    "\n",
    "for key in a1.keys():\n",
    "    if len(np.unique(a1[key]))>1:\n",
    "        accept1.append(key)\n",
    "for key in a2.keys():\n",
    "    if len(np.unique(a2[key]))>1:\n",
    "        accept2.append(key) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normarlize(length):\n",
    "    return (length-length.mean())/length.std(),length.mean(),length.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 116.55it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 296.57it/s]\n"
     ]
    }
   ],
   "source": [
    "b1=list()\n",
    "b2=list()\n",
    "des=dict()\n",
    "accept2=np.delete(accept2,np.argwhere(accept2=='label'))\n",
    "for key in tqdm(sorted(accept2)):\n",
    "    if a2[key].dtype=='int64' or a2[key].dtype=='float64':\n",
    "        nm,mean,std=normarlize(a2[key])\n",
    "        nm=np.expand_dims(nm.tolist(),0).T\n",
    "        des[key]=[mean,std]\n",
    "    elif a2[key].dtype=='O':\n",
    "        uni=a2[key].unique()\n",
    "        l=len(uni)\n",
    "        if '-1' in uni:\n",
    "            l-=1\n",
    "        nm=np.zeros((len(a2[key]),l))\n",
    "        uni=np.delete(uni,np.argwhere(uni=='-1'))\n",
    "        uni=dict(zip(uni,list(range(len(uni)))))\n",
    "        des[key]=uni\n",
    "        for i,n in enumerate(a2[key]):\n",
    "            try:\n",
    "                nm[i,uni[n]]=1\n",
    "            except:\n",
    "                continue  \n",
    "    else:\n",
    "        continue\n",
    "    b2.append(nm)    \n",
    "    \n",
    "for key in tqdm(sorted(accept2)):\n",
    "    if a1[key].dtype=='int64' or a1[key].dtype=='float64':\n",
    "        nm=(a1[key]-des[key][0])/des[key][1]\n",
    "        nm=np.expand_dims(nm.tolist(),0).T\n",
    "    elif a1[key].dtype=='O':\n",
    "        nm=np.zeros((len(a1[key]),len(des[key].keys())))\n",
    "        for i,n in enumerate(a1[key]):\n",
    "            try:\n",
    "                nm[i,des[key][n]]=1\n",
    "            except:\n",
    "                continue  \n",
    "    else:\n",
    "        continue\n",
    "    b1.append(nm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=np.concatenate(b2,axis=1)\n",
    "data_test=np.concatenate(b1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier =SVC(kernel='linear',verbose=True,C=5)\n",
    "svclassifier.fit(data_train,a2.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91     50400\n",
      "           1       1.00      0.43      0.60     17545\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     67945\n",
      "   macro avg       0.92      0.72      0.76     67945\n",
      "weighted avg       0.88      0.85      0.83     67945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91     21600\n",
      "           1       1.00      0.43      0.60      7520\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     29120\n",
      "   macro avg       0.92      0.71      0.75     29120\n",
      "weighted avg       0.88      0.85      0.83     29120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred=svclassifier.predict(data_train)\n",
    "print(classification_report(a2.label, pred))\n",
    "pred=svclassifier.predict(data_test)\n",
    "print(classification_report(a1.label, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model fail\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "layer_ids = ['hidden1','hidden2','out']\n",
    "layer_sizes = [data_train.shape[1], 200,150, 2]\n",
    "train_inputs = tf.placeholder(tf.float32, shape=[None, layer_sizes[0]], name='train_inputs')\n",
    "train_labels = tf.placeholder(tf.int32, shape=[None], name='train_labels')\n",
    "for idx, lid in enumerate(layer_ids):\n",
    "    with tf.variable_scope(lid):\n",
    "        w = tf.get_variable('weights',shape=[layer_sizes[idx], layer_sizes[idx+1]],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "        b = tf.get_variable('bias',shape= [layer_sizes[idx+1]],\n",
    "                            initializer=tf.random_uniform_initializer(-0.1,0.1))\n",
    "h = train_inputs\n",
    "for lid in layer_ids:\n",
    "    with tf.variable_scope(lid,reuse=True):\n",
    "        w, b = tf.get_variable('weights'), tf.get_variable('bias')\n",
    "        if lid != 'out':\n",
    "            h = tf.nn.relu(tf.matmul(h,w)+b,name=lid+'_output')\n",
    "        else:\n",
    "            h = tf.nn.xw_plus_b(h,w,b,name=lid+'_output')\n",
    "tf_predictions = tf.nn.softmax(h, name='predictions')\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(train_labels,depth=2), logits=h),name='loss')\n",
    "tf_learning_rate = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001,).minimize(tf_loss)\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoint/model-999\n",
      "Load complete at step:  1000\n",
      "Step: 1000 loss: 20.189390 \n",
      "Step: 1001 loss: 40.842369 \n",
      "Step: 1002 loss: 18.268194 \n",
      "Step: 1003 loss: 19.314547 \n",
      "Step: 1004 loss: 19.758511 \n",
      "Step: 1005 loss: 19.099962 \n",
      "Step: 1006 loss: 18.420989 \n",
      "Step: 1007 loss: 17.300523 \n",
      "Step: 1008 loss: 17.538438 \n",
      "Step: 1009 loss: 17.052180 \n",
      "Accuracy:  0.8737980769230769\n",
      "Step: 1010 loss: 16.837553 \n",
      "Step: 1011 loss: 16.566855 \n",
      "Step: 1012 loss: 16.498178 \n",
      "Step: 1013 loss: 16.452642 \n",
      "Step: 1014 loss: 16.441703 \n",
      "Step: 1015 loss: 16.412061 \n",
      "Step: 1016 loss: 16.371610 \n",
      "Step: 1017 loss: 16.385583 \n",
      "Step: 1018 loss: 16.326042 \n",
      "Step: 1019 loss: 16.383528 \n",
      "Accuracy:  0.8769917582417582\n",
      "Step: 1020 loss: 16.309437 \n",
      "Step: 1021 loss: 16.487015 \n",
      "Step: 1022 loss: 16.319016 \n",
      "Step: 1023 loss: 16.422553 \n",
      "Step: 1024 loss: 16.233277 \n",
      "Step: 1025 loss: 16.333511 \n",
      "Step: 1026 loss: 16.163591 \n",
      "Step: 1027 loss: 16.262859 \n",
      "Step: 1028 loss: 16.206428 \n",
      "Step: 1029 loss: 16.351854 \n",
      "Accuracy:  0.8770947802197803\n",
      "Step: 1030 loss: 16.350833 \n",
      "Step: 1031 loss: 16.502868 \n",
      "Step: 1032 loss: 16.222829 \n",
      "Step: 1033 loss: 16.241283 \n",
      "Step: 1034 loss: 16.166235 \n",
      "Step: 1035 loss: 16.326775 \n",
      "Step: 1036 loss: 16.154232 \n",
      "Step: 1037 loss: 16.306887 \n",
      "Step: 1038 loss: 16.072848 \n",
      "Step: 1039 loss: 16.219990 \n",
      "Accuracy:  0.8774038461538461\n",
      "Step: 1040 loss: 16.106244 \n",
      "Step: 1041 loss: 16.221704 \n",
      "Step: 1042 loss: 16.304500 \n",
      "Step: 1043 loss: 16.429605 \n",
      "Step: 1044 loss: 16.129221 \n",
      "Step: 1045 loss: 16.245477 \n",
      "Step: 1046 loss: 16.099308 \n",
      "Step: 1047 loss: 16.185961 \n",
      "Step: 1048 loss: 16.116453 \n",
      "Step: 1049 loss: 16.162354 \n",
      "Accuracy:  0.8773351648351648\n",
      "Step: 1050 loss: 16.087932 \n",
      "Step: 1051 loss: 16.112066 \n",
      "Step: 1052 loss: 16.073841 \n",
      "Step: 1053 loss: 16.062743 \n",
      "Step: 1054 loss: 16.163329 \n",
      "Step: 1055 loss: 16.080326 \n",
      "Step: 1056 loss: 16.064947 \n",
      "Step: 1057 loss: 16.068058 \n",
      "Step: 1058 loss: 16.116762 \n",
      "Step: 1059 loss: 16.048623 \n",
      "Accuracy:  0.8765796703296703\n",
      "Step: 1060 loss: 16.011309 \n",
      "Step: 1061 loss: 16.034441 \n",
      "Step: 1062 loss: 16.052416 \n",
      "Step: 1063 loss: 15.964173 \n",
      "Step: 1064 loss: 16.118490 \n",
      "Step: 1065 loss: 16.002102 \n",
      "Step: 1066 loss: 16.073197 \n",
      "Step: 1067 loss: 16.018762 \n",
      "Step: 1068 loss: 16.089528 \n",
      "Step: 1069 loss: 16.000528 \n",
      "Accuracy:  0.8761675824175824\n",
      "Step: 1070 loss: 16.058333 \n",
      "Step: 1071 loss: 16.017055 \n",
      "Step: 1072 loss: 15.972825 \n",
      "Step: 1073 loss: 15.986473 \n",
      "Step: 1074 loss: 15.948320 \n",
      "Step: 1075 loss: 16.091229 \n",
      "Step: 1076 loss: 16.103417 \n",
      "Step: 1077 loss: 16.230560 \n",
      "Step: 1078 loss: 16.274375 \n",
      "Step: 1079 loss: 16.022628 \n",
      "Accuracy:  0.8777815934065935\n",
      "Step: 1080 loss: 16.568018 \n",
      "Step: 1081 loss: 16.922614 \n",
      "Step: 1082 loss: 16.462659 \n",
      "Step: 1083 loss: 16.311964 \n",
      "Step: 1084 loss: 16.212389 \n",
      "Step: 1085 loss: 16.059976 \n",
      "Step: 1086 loss: 16.002065 \n",
      "Step: 1087 loss: 16.053655 \n",
      "Step: 1088 loss: 16.505028 \n",
      "Step: 1089 loss: 16.905486 \n",
      "Accuracy:  0.8718406593406594\n",
      "Step: 1090 loss: 16.479522 \n",
      "Step: 1091 loss: 16.081139 \n",
      "Step: 1092 loss: 16.099610 \n",
      "Step: 1093 loss: 16.129245 \n",
      "Step: 1094 loss: 16.017074 \n",
      "Step: 1095 loss: 15.922396 \n",
      "Step: 1096 loss: 15.980781 \n",
      "Step: 1097 loss: 15.866022 \n",
      "Step: 1098 loss: 15.894454 \n",
      "Step: 1099 loss: 15.874407 \n",
      "Accuracy:  0.8779532967032967\n",
      "Step: 1100 loss: 15.881756 \n",
      "Step: 1101 loss: 15.923636 \n",
      "Step: 1102 loss: 15.989127 \n",
      "Step: 1103 loss: 15.861969 \n",
      "Step: 1104 loss: 15.861242 \n",
      "Step: 1105 loss: 15.901158 \n",
      "Step: 1106 loss: 15.889795 \n",
      "Step: 1107 loss: 16.037441 \n",
      "Step: 1108 loss: 16.560861 \n",
      "Step: 1109 loss: 20.123013 \n",
      "Accuracy:  0.8628777472527472\n",
      "Step: 1110 loss: 19.945751 \n",
      "Step: 1111 loss: 26.039839 \n",
      "Step: 1112 loss: 19.319630 \n",
      "Step: 1113 loss: 18.522842 \n",
      "Step: 1114 loss: 18.456749 \n",
      "Step: 1115 loss: 18.321116 \n",
      "Step: 1116 loss: 18.387313 \n",
      "Step: 1117 loss: 18.502021 \n",
      "Step: 1118 loss: 18.294485 \n",
      "Step: 1119 loss: 18.453037 \n",
      "Accuracy:  0.8685096153846154\n",
      "Step: 1120 loss: 18.215384 \n",
      "Step: 1121 loss: 18.521201 \n",
      "Step: 1122 loss: 18.098912 \n",
      "Step: 1123 loss: 18.012988 \n",
      "Step: 1124 loss: 19.289394 \n",
      "Step: 1125 loss: 16.731878 \n",
      "Step: 1126 loss: 29.899442 \n",
      "Step: 1127 loss: 20.808489 \n",
      "Step: 1128 loss: 18.303945 \n",
      "Step: 1129 loss: 17.597220 \n",
      "Accuracy:  0.867135989010989\n",
      "Step: 1130 loss: 17.573339 \n",
      "Step: 1131 loss: 18.089799 \n",
      "Step: 1132 loss: 17.228164 \n",
      "Step: 1133 loss: 18.243469 \n",
      "Step: 1134 loss: 17.764915 \n",
      "Step: 1135 loss: 22.935727 \n",
      "Step: 1136 loss: 17.770831 \n",
      "Step: 1137 loss: 17.946387 \n",
      "Step: 1138 loss: 17.106868 \n",
      "Step: 1139 loss: 17.110621 \n",
      "Accuracy:  0.8732486263736263\n",
      "Step: 1140 loss: 16.698961 \n",
      "Step: 1141 loss: 16.266868 \n",
      "Step: 1142 loss: 16.157246 \n",
      "Step: 1143 loss: 16.149810 \n",
      "Step: 1144 loss: 16.138417 \n",
      "Step: 1145 loss: 16.145537 \n",
      "Step: 1146 loss: 16.105043 \n",
      "Step: 1147 loss: 16.129741 \n",
      "Step: 1148 loss: 16.094458 \n",
      "Step: 1149 loss: 16.132042 \n",
      "Accuracy:  0.8787431318681319\n",
      "Step: 1150 loss: 16.108647 \n",
      "Step: 1151 loss: 16.207071 \n",
      "Step: 1152 loss: 16.043870 \n",
      "Step: 1153 loss: 16.118635 \n",
      "Step: 1154 loss: 16.075612 \n",
      "Step: 1155 loss: 16.079902 \n",
      "Step: 1156 loss: 16.040857 \n",
      "Step: 1157 loss: 16.145167 \n",
      "Step: 1158 loss: 16.099468 \n",
      "Step: 1159 loss: 16.260289 \n",
      "Accuracy:  0.878331043956044\n",
      "Step: 1160 loss: 16.020275 \n",
      "Step: 1161 loss: 16.125680 \n",
      "Step: 1162 loss: 16.026007 \n",
      "Step: 1163 loss: 16.013230 \n",
      "Step: 1164 loss: 16.029176 \n",
      "Step: 1165 loss: 16.191895 \n",
      "Step: 1166 loss: 16.067045 \n",
      "Step: 1167 loss: 15.916785 \n",
      "Step: 1168 loss: 15.991252 \n",
      "Step: 1169 loss: 15.918817 \n",
      "Accuracy:  0.8785027472527472\n",
      "Step: 1170 loss: 15.928889 \n",
      "Step: 1171 loss: 15.874423 \n",
      "Step: 1172 loss: 15.903210 \n",
      "Step: 1173 loss: 15.881086 \n",
      "Step: 1174 loss: 15.809303 \n",
      "Step: 1175 loss: 20.678654 \n",
      "Step: 1176 loss: 19.503532 \n",
      "Step: 1177 loss: 19.836191 \n",
      "Step: 1178 loss: 18.688353 \n",
      "Step: 1179 loss: 18.743318 \n",
      "Accuracy:  0.8633241758241759\n",
      "Step: 1180 loss: 18.460289 \n",
      "Step: 1181 loss: 18.370891 \n",
      "Step: 1182 loss: 18.227408 \n",
      "Step: 1183 loss: 18.413056 \n",
      "Step: 1184 loss: 18.112522 \n",
      "Step: 1185 loss: 17.904264 \n",
      "Step: 1186 loss: 17.790092 \n",
      "Step: 1187 loss: 17.878819 \n",
      "Step: 1188 loss: 17.627996 \n",
      "Step: 1189 loss: 17.843914 \n",
      "Accuracy:  0.8669642857142857\n",
      "Step: 1190 loss: 17.528263 \n",
      "Step: 1191 loss: 17.444089 \n",
      "Step: 1192 loss: 17.337801 \n",
      "Step: 1193 loss: 17.312477 \n",
      "Step: 1194 loss: 17.583437 \n",
      "Step: 1195 loss: 17.694063 \n",
      "Step: 1196 loss: 17.503297 \n",
      "Step: 1197 loss: 17.286180 \n",
      "Step: 1198 loss: 17.069140 \n",
      "Step: 1199 loss: 17.308753 \n",
      "Accuracy:  0.8728708791208791\n",
      "Step: 1200 loss: 16.642252 \n",
      "Step: 1201 loss: 16.399978 \n",
      "Step: 1202 loss: 16.184705 \n",
      "Step: 1203 loss: 16.102059 \n",
      "Step: 1204 loss: 16.127004 \n",
      "Step: 1205 loss: 16.183134 \n",
      "Step: 1206 loss: 16.535646 \n",
      "Step: 1207 loss: 16.216220 \n",
      "Step: 1208 loss: 16.049904 \n",
      "Step: 1209 loss: 15.951923 \n",
      "Accuracy:  0.8793269230769231\n",
      "Step: 1210 loss: 15.981169 \n",
      "Step: 1211 loss: 15.886374 \n",
      "Step: 1212 loss: 15.987972 \n",
      "Step: 1213 loss: 15.821015 \n",
      "Step: 1214 loss: 16.041048 \n",
      "Step: 1215 loss: 15.775142 \n",
      "Step: 1216 loss: 16.165161 \n",
      "Step: 1217 loss: 16.139195 \n",
      "Step: 1218 loss: 16.536942 \n",
      "Step: 1219 loss: 16.001613 \n",
      "Accuracy:  0.8779189560439561\n",
      "Step: 1220 loss: 16.142255 \n",
      "Step: 1221 loss: 15.849713 \n",
      "Step: 1222 loss: 16.018348 \n",
      "Step: 1223 loss: 15.757403 \n",
      "Step: 1224 loss: 16.071607 \n",
      "Step: 1225 loss: 16.071176 \n",
      "Step: 1226 loss: 16.608316 \n",
      "Step: 1227 loss: 16.110652 \n",
      "Step: 1228 loss: 16.073715 \n",
      "Step: 1229 loss: 15.972669 \n",
      "Accuracy:  0.8790521978021978\n",
      "Step: 1230 loss: 16.013679 \n",
      "Step: 1231 loss: 15.795161 \n",
      "Step: 1232 loss: 16.038586 \n",
      "Step: 1233 loss: 15.828835 \n",
      "Step: 1234 loss: 16.157616 \n",
      "Step: 1235 loss: 16.084393 \n",
      "Step: 1236 loss: 16.013578 \n",
      "Step: 1237 loss: 15.807058 \n",
      "Step: 1238 loss: 15.851976 \n",
      "Step: 1239 loss: 15.812609 \n",
      "Accuracy:  0.8793956043956044\n",
      "Step: 1240 loss: 15.812671 \n",
      "Step: 1241 loss: 15.788395 \n",
      "Step: 1242 loss: 15.771780 \n",
      "Step: 1243 loss: 15.813334 \n",
      "Step: 1244 loss: 15.820510 \n",
      "Step: 1245 loss: 15.777872 \n",
      "Step: 1246 loss: 15.815632 \n",
      "Step: 1247 loss: 15.738127 \n",
      "Step: 1248 loss: 16.202684 \n",
      "Step: 1249 loss: 18.584278 \n",
      "Accuracy:  0.8683722527472527\n",
      "Step: 1250 loss: 18.219656 \n",
      "Step: 1251 loss: 21.510962 \n",
      "Step: 1252 loss: 20.409549 \n",
      "Step: 1253 loss: 19.713053 \n",
      "Step: 1254 loss: 17.284374 \n",
      "Step: 1255 loss: 25.459945 \n",
      "Step: 1256 loss: 18.642707 \n",
      "Step: 1257 loss: 18.137726 \n",
      "Step: 1258 loss: 17.916942 \n",
      "Step: 1259 loss: 17.772915 \n",
      "Accuracy:  0.876717032967033\n",
      "Step: 1260 loss: 17.274698 \n",
      "Step: 1261 loss: 17.341776 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1262 loss: 17.406027 \n",
      "Step: 1263 loss: 17.173646 \n",
      "Step: 1264 loss: 17.200244 \n",
      "Step: 1265 loss: 17.370348 \n",
      "Step: 1266 loss: 17.144940 \n",
      "Step: 1267 loss: 17.164632 \n",
      "Step: 1268 loss: 17.438784 \n",
      "Step: 1269 loss: 17.055088 \n",
      "Accuracy:  0.8755494505494505\n",
      "Step: 1270 loss: 17.153149 \n",
      "Step: 1271 loss: 16.568653 \n",
      "Step: 1272 loss: 17.390942 \n",
      "Step: 1273 loss: 17.897695 \n",
      "Step: 1274 loss: 17.658264 \n",
      "Step: 1275 loss: 16.982225 \n",
      "Step: 1276 loss: 17.165690 \n",
      "Step: 1277 loss: 17.610783 \n",
      "Step: 1278 loss: 16.341433 \n",
      "Step: 1279 loss: 16.363711 \n",
      "Accuracy:  0.8750343406593407\n",
      "Step: 1280 loss: 16.351997 \n",
      "Step: 1281 loss: 16.487973 \n",
      "Step: 1282 loss: 16.265102 \n",
      "Step: 1283 loss: 15.944338 \n",
      "Step: 1284 loss: 16.014268 \n",
      "Step: 1285 loss: 16.186053 \n",
      "Step: 1286 loss: 15.916451 \n",
      "Step: 1287 loss: 15.852213 \n",
      "Step: 1288 loss: 15.859014 \n",
      "Step: 1289 loss: 15.815665 \n",
      "Accuracy:  0.8789148351648352\n",
      "Step: 1290 loss: 15.803894 \n",
      "Step: 1291 loss: 15.785903 \n",
      "Step: 1292 loss: 15.746865 \n",
      "Step: 1293 loss: 15.714826 \n",
      "Step: 1294 loss: 15.667733 \n",
      "Step: 1295 loss: 15.669760 \n",
      "Step: 1296 loss: 15.639920 \n",
      "Step: 1297 loss: 15.626121 \n",
      "Step: 1298 loss: 15.634311 \n",
      "Step: 1299 loss: 15.615356 \n",
      "Accuracy:  0.8792925824175825\n",
      "Step: 1300 loss: 15.674113 \n",
      "Step: 1301 loss: 15.604869 \n",
      "Step: 1302 loss: 15.613115 \n",
      "Step: 1303 loss: 15.565959 \n",
      "Step: 1304 loss: 15.595539 \n",
      "Step: 1305 loss: 15.583561 \n",
      "Step: 1306 loss: 15.594527 \n",
      "Step: 1307 loss: 15.647876 \n",
      "Step: 1308 loss: 15.561432 \n",
      "Step: 1309 loss: 15.749121 \n",
      "Accuracy:  0.8788461538461538\n",
      "Step: 1310 loss: 15.711309 \n",
      "Step: 1311 loss: 15.652603 \n",
      "Step: 1312 loss: 15.619426 \n",
      "Step: 1313 loss: 15.597150 \n",
      "Step: 1314 loss: 15.624148 \n",
      "Step: 1315 loss: 15.585282 \n",
      "Step: 1316 loss: 15.623568 \n",
      "Step: 1317 loss: 15.649862 \n",
      "Step: 1318 loss: 15.937714 \n",
      "Step: 1319 loss: 15.884992 \n",
      "Accuracy:  0.8795673076923077\n",
      "Step: 1320 loss: 15.665353 \n",
      "Step: 1321 loss: 15.636864 \n",
      "Step: 1322 loss: 15.573841 \n",
      "Step: 1323 loss: 15.557861 \n",
      "Step: 1324 loss: 15.605382 \n",
      "Step: 1325 loss: 15.539980 \n",
      "Step: 1326 loss: 15.637708 \n",
      "Step: 1327 loss: 15.917918 \n",
      "Step: 1328 loss: 16.759940 \n",
      "Step: 1329 loss: 17.347102 \n",
      "Accuracy:  0.8637706043956044\n",
      "Step: 1330 loss: 26.818194 \n",
      "Step: 1331 loss: 116.725178 \n",
      "Step: 1332 loss: 24.446378 \n",
      "Step: 1333 loss: 34.874861 \n",
      "Step: 1334 loss: 21.497841 \n",
      "Step: 1335 loss: 19.288473 \n",
      "Step: 1336 loss: 22.321588 \n",
      "Step: 1337 loss: 19.270025 \n",
      "Step: 1338 loss: 19.161998 \n",
      "Step: 1339 loss: 19.103926 \n",
      "Accuracy:  0.8523008241758242\n",
      "Step: 1340 loss: 18.913847 \n",
      "Step: 1341 loss: 18.696766 \n",
      "Step: 1342 loss: 18.912476 \n",
      "Step: 1343 loss: 18.479403 \n",
      "Step: 1344 loss: 18.647829 \n",
      "Step: 1345 loss: 18.536614 \n",
      "Step: 1346 loss: 22.268746 \n",
      "Step: 1347 loss: 19.188054 \n",
      "Step: 1348 loss: 18.501227 \n",
      "Step: 1349 loss: 17.858566 \n",
      "Accuracy:  0.8691277472527472\n",
      "Step: 1350 loss: 17.766960 \n",
      "Step: 1351 loss: 17.633312 \n",
      "Step: 1352 loss: 17.436196 \n",
      "Step: 1353 loss: 17.414271 \n",
      "Step: 1354 loss: 17.233642 \n",
      "Step: 1355 loss: 17.201452 \n",
      "Step: 1356 loss: 17.822750 \n",
      "Step: 1357 loss: 17.633608 \n",
      "Step: 1358 loss: 17.032736 \n",
      "Step: 1359 loss: 17.389350 \n",
      "Accuracy:  0.8671016483516484\n",
      "Step: 1360 loss: 17.274363 \n",
      "Step: 1361 loss: 17.486199 \n",
      "Step: 1362 loss: 16.995769 \n",
      "Step: 1363 loss: 16.600595 \n",
      "Step: 1364 loss: 16.669043 \n",
      "Step: 1365 loss: 16.710623 \n",
      "Step: 1366 loss: 16.513884 \n",
      "Step: 1367 loss: 16.791750 \n",
      "Step: 1368 loss: 16.819685 \n",
      "Step: 1369 loss: 16.729367 \n",
      "Accuracy:  0.8739354395604395\n",
      "Step: 1370 loss: 16.436052 \n",
      "Step: 1371 loss: 16.701007 \n",
      "Step: 1372 loss: 16.617270 \n",
      "Step: 1373 loss: 16.491307 \n",
      "Step: 1374 loss: 16.361626 \n",
      "Step: 1375 loss: 16.665240 \n",
      "Step: 1376 loss: 16.435706 \n",
      "Step: 1377 loss: 16.333743 \n",
      "Step: 1378 loss: 16.341737 \n",
      "Step: 1379 loss: 16.417024 \n",
      "Accuracy:  0.8747252747252747\n",
      "Step: 1380 loss: 16.240708 \n",
      "Step: 1381 loss: 16.512258 \n",
      "Step: 1382 loss: 16.389223 \n",
      "Step: 1383 loss: 16.253006 \n",
      "Step: 1384 loss: 16.243885 \n",
      "Step: 1385 loss: 16.394559 \n",
      "Step: 1386 loss: 16.266701 \n",
      "Step: 1387 loss: 16.135829 \n",
      "Step: 1388 loss: 16.186113 \n",
      "Step: 1389 loss: 16.198287 \n",
      "Accuracy:  0.873179945054945\n",
      "Step: 1390 loss: 16.401745 \n",
      "Step: 1391 loss: 16.238526 \n",
      "Step: 1392 loss: 16.143194 \n",
      "Step: 1393 loss: 16.159794 \n",
      "Step: 1394 loss: 16.115603 \n",
      "Step: 1395 loss: 16.196400 \n",
      "Step: 1396 loss: 16.774936 \n",
      "Step: 1397 loss: 21.202614 \n",
      "Step: 1398 loss: 16.830296 \n",
      "Step: 1399 loss: 16.304682 \n",
      "Accuracy:  0.8737637362637363\n",
      "Step: 1400 loss: 16.309181 \n",
      "Step: 1401 loss: 16.255864 \n",
      "Step: 1402 loss: 16.120280 \n",
      "Step: 1403 loss: 16.391102 \n",
      "Step: 1404 loss: 16.130771 \n",
      "Step: 1405 loss: 16.193399 \n",
      "Step: 1406 loss: 15.936087 \n",
      "Step: 1407 loss: 15.861921 \n",
      "Step: 1408 loss: 15.844431 \n",
      "Step: 1409 loss: 15.815603 \n",
      "Accuracy:  0.8786744505494506\n",
      "Step: 1410 loss: 15.814428 \n",
      "Step: 1411 loss: 15.763048 \n",
      "Step: 1412 loss: 15.804116 \n",
      "Step: 1413 loss: 15.807868 \n",
      "Step: 1414 loss: 16.682724 \n",
      "Step: 1415 loss: 16.845558 \n",
      "Step: 1416 loss: 16.314672 \n",
      "Step: 1417 loss: 16.072664 \n",
      "Step: 1418 loss: 15.783151 \n",
      "Step: 1419 loss: 18.245226 \n",
      "Accuracy:  0.8649381868131868\n",
      "Step: 1420 loss: 19.940793 \n",
      "Step: 1421 loss: 18.515174 \n",
      "Step: 1422 loss: 20.591265 \n",
      "Step: 1423 loss: 17.655392 \n",
      "Step: 1424 loss: 17.334697 \n",
      "Step: 1425 loss: 17.299022 \n",
      "Step: 1426 loss: 16.678437 \n",
      "Step: 1427 loss: 16.742799 \n",
      "Step: 1428 loss: 16.588395 \n",
      "Step: 1429 loss: 16.196683 \n",
      "Accuracy:  0.875\n",
      "Step: 1430 loss: 16.328715 \n",
      "Step: 1431 loss: 16.400669 \n",
      "Step: 1432 loss: 16.764076 \n",
      "Step: 1433 loss: 16.497656 \n",
      "Step: 1434 loss: 16.255275 \n",
      "Step: 1435 loss: 15.996646 \n",
      "Step: 1436 loss: 15.867289 \n",
      "Step: 1437 loss: 15.679273 \n",
      "Step: 1438 loss: 15.851602 \n",
      "Step: 1439 loss: 15.628044 \n",
      "Accuracy:  0.8788118131868132\n",
      "Step: 1440 loss: 15.737305 \n",
      "Step: 1441 loss: 15.610816 \n",
      "Step: 1442 loss: 15.799380 \n",
      "Step: 1443 loss: 15.581875 \n",
      "Step: 1444 loss: 15.638101 \n",
      "Step: 1445 loss: 15.629866 \n",
      "Step: 1446 loss: 15.797277 \n",
      "Step: 1447 loss: 15.581007 \n",
      "Step: 1448 loss: 15.608834 \n",
      "Step: 1449 loss: 15.540431 \n",
      "Accuracy:  0.8787774725274725\n",
      "Step: 1450 loss: 15.675293 \n",
      "Step: 1451 loss: 15.638246 \n",
      "Step: 1452 loss: 15.821221 \n",
      "Step: 1453 loss: 15.627679 \n",
      "Step: 1454 loss: 15.554241 \n",
      "Step: 1455 loss: 15.559679 \n",
      "Step: 1456 loss: 15.545348 \n",
      "Step: 1457 loss: 15.609254 \n",
      "Step: 1458 loss: 15.697340 \n",
      "Step: 1459 loss: 15.847355 \n",
      "Accuracy:  0.8799793956043956\n",
      "Step: 1460 loss: 15.686140 \n",
      "Step: 1461 loss: 15.527125 \n",
      "Step: 1462 loss: 15.539833 \n",
      "Step: 1463 loss: 15.565655 \n",
      "Step: 1464 loss: 15.655088 \n",
      "Step: 1465 loss: 15.554522 \n",
      "Step: 1466 loss: 15.553767 \n",
      "Step: 1467 loss: 15.598999 \n",
      "Step: 1468 loss: 15.666421 \n",
      "Step: 1469 loss: 15.512623 \n",
      "Accuracy:  0.8794986263736264\n",
      "Step: 1470 loss: 15.598016 \n",
      "Step: 1471 loss: 15.580464 \n",
      "Step: 1472 loss: 15.613271 \n",
      "Step: 1473 loss: 15.625099 \n",
      "Step: 1474 loss: 15.736435 \n",
      "Step: 1475 loss: 15.563912 \n",
      "Step: 1476 loss: 15.521053 \n",
      "Step: 1477 loss: 15.551602 \n",
      "Step: 1478 loss: 15.550398 \n",
      "Step: 1479 loss: 15.582297 \n",
      "Accuracy:  0.8770604395604396\n",
      "Step: 1480 loss: 15.739560 \n",
      "Step: 1481 loss: 15.598756 \n",
      "Step: 1482 loss: 15.567605 \n",
      "Step: 1483 loss: 15.597532 \n",
      "Step: 1484 loss: 15.703851 \n",
      "Step: 1485 loss: 16.072722 \n",
      "Step: 1486 loss: 17.155336 \n",
      "Step: 1487 loss: 17.854287 \n",
      "Step: 1488 loss: 20.232975 \n",
      "Step: 1489 loss: 17.464063 \n",
      "Accuracy:  0.8632211538461538\n",
      "Step: 1490 loss: 21.873748 \n",
      "Step: 1491 loss: 18.695177 \n",
      "Step: 1492 loss: 18.278272 \n",
      "Step: 1493 loss: 18.203878 \n",
      "Step: 1494 loss: 18.084089 \n",
      "Step: 1495 loss: 18.257598 \n",
      "Step: 1496 loss: 18.012627 \n",
      "Step: 1497 loss: 18.099564 \n",
      "Step: 1498 loss: 17.557315 \n",
      "Step: 1499 loss: 17.676586 \n",
      "Accuracy:  0.8679601648351648\n",
      "Step: 1500 loss: 19.094132 \n",
      "Step: 1501 loss: 18.351573 \n",
      "Step: 1502 loss: 20.019119 \n",
      "Step: 1503 loss: 17.966511 \n",
      "Step: 1504 loss: 17.798422 \n",
      "Step: 1505 loss: 17.089638 \n",
      "Step: 1506 loss: 16.869136 \n",
      "Step: 1507 loss: 16.833846 \n",
      "Step: 1508 loss: 16.689771 \n",
      "Step: 1509 loss: 16.918206 \n",
      "Accuracy:  0.8775068681318682\n",
      "Step: 1510 loss: 16.760442 \n",
      "Step: 1511 loss: 16.654190 \n",
      "Step: 1512 loss: 16.532084 \n",
      "Step: 1513 loss: 16.635918 \n",
      "Step: 1514 loss: 16.334190 \n",
      "Step: 1515 loss: 17.173534 \n",
      "Step: 1516 loss: 16.564442 \n",
      "Step: 1517 loss: 16.317871 \n",
      "Step: 1518 loss: 16.228268 \n",
      "Step: 1519 loss: 17.954599 \n",
      "Accuracy:  0.8675824175824176\n",
      "Step: 1520 loss: 16.766246 \n",
      "Step: 1521 loss: 83.447424 \n",
      "Step: 1522 loss: 17.433653 \n",
      "Step: 1523 loss: 23.892381 \n",
      "Step: 1524 loss: 19.429731 \n",
      "Step: 1525 loss: 18.977163 \n",
      "Step: 1526 loss: 18.616017 \n",
      "Step: 1527 loss: 18.154575 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1528 loss: 17.560623 \n",
      "Step: 1529 loss: 17.135113 \n",
      "Accuracy:  0.8744505494505495\n",
      "Step: 1530 loss: 17.096714 \n",
      "Step: 1531 loss: 17.054730 \n",
      "Step: 1532 loss: 16.984282 \n",
      "Step: 1533 loss: 16.955055 \n",
      "Step: 1534 loss: 16.935493 \n",
      "Step: 1535 loss: 16.913369 \n",
      "Step: 1536 loss: 16.886785 \n",
      "Step: 1537 loss: 16.822911 \n",
      "Step: 1538 loss: 16.821576 \n",
      "Step: 1539 loss: 16.641502 \n",
      "Accuracy:  0.8747596153846153\n",
      "Step: 1540 loss: 17.399381 \n",
      "Step: 1541 loss: 16.725178 \n",
      "Step: 1542 loss: 16.777952 \n",
      "Step: 1543 loss: 16.553013 \n",
      "Step: 1544 loss: 16.530762 \n",
      "Step: 1545 loss: 16.464732 \n",
      "Step: 1546 loss: 16.452113 \n",
      "Step: 1547 loss: 16.437690 \n",
      "Step: 1548 loss: 16.488995 \n",
      "Step: 1549 loss: 16.385324 \n",
      "Accuracy:  0.8767857142857143\n",
      "Step: 1550 loss: 16.374429 \n",
      "Step: 1551 loss: 16.341505 \n",
      "Step: 1552 loss: 16.315872 \n",
      "Step: 1553 loss: 16.308138 \n",
      "Step: 1554 loss: 16.295424 \n",
      "Step: 1555 loss: 16.281165 \n",
      "Step: 1556 loss: 16.241115 \n",
      "Step: 1557 loss: 16.256019 \n",
      "Step: 1558 loss: 16.235817 \n",
      "Step: 1559 loss: 16.228495 \n",
      "Accuracy:  0.8769574175824176\n",
      "Step: 1560 loss: 16.235914 \n",
      "Step: 1561 loss: 16.155495 \n",
      "Step: 1562 loss: 16.166700 \n",
      "Step: 1563 loss: 16.176632 \n",
      "Step: 1564 loss: 16.120193 \n",
      "Step: 1565 loss: 16.140049 \n",
      "Step: 1566 loss: 16.071356 \n",
      "Step: 1567 loss: 16.070342 \n",
      "Step: 1568 loss: 16.036686 \n",
      "Step: 1569 loss: 16.048448 \n",
      "Accuracy:  0.8775755494505495\n",
      "Step: 1570 loss: 16.055596 \n",
      "Step: 1571 loss: 15.973155 \n",
      "Step: 1572 loss: 16.091447 \n",
      "Step: 1573 loss: 16.219719 \n",
      "Step: 1574 loss: 16.379378 \n",
      "Step: 1575 loss: 16.267806 \n",
      "Step: 1576 loss: 16.081905 \n",
      "Step: 1577 loss: 16.113327 \n",
      "Step: 1578 loss: 16.247286 \n",
      "Step: 1579 loss: 16.168077 \n",
      "Accuracy:  0.8751373626373626\n",
      "Step: 1580 loss: 16.147111 \n",
      "Step: 1581 loss: 15.999619 \n",
      "Step: 1582 loss: 15.932886 \n",
      "Step: 1583 loss: 15.910431 \n",
      "Step: 1584 loss: 15.969055 \n",
      "Step: 1585 loss: 16.057783 \n",
      "Step: 1586 loss: 16.198018 \n",
      "Step: 1587 loss: 15.970114 \n",
      "Step: 1588 loss: 15.897718 \n",
      "Step: 1589 loss: 15.906934 \n",
      "Accuracy:  0.8790178571428572\n",
      "Step: 1590 loss: 15.812476 \n",
      "Step: 1591 loss: 15.889913 \n",
      "Step: 1592 loss: 16.064564 \n",
      "Step: 1593 loss: 16.208311 \n",
      "Step: 1594 loss: 16.031946 \n",
      "Step: 1595 loss: 15.837548 \n",
      "Step: 1596 loss: 15.880040 \n",
      "Step: 1597 loss: 15.775598 \n",
      "Step: 1598 loss: 15.910217 \n",
      "Step: 1599 loss: 15.803981 \n",
      "Accuracy:  0.8763736263736264\n",
      "Step: 1600 loss: 15.985793 \n",
      "Step: 1601 loss: 15.919673 \n",
      "Step: 1602 loss: 16.166415 \n",
      "Step: 1603 loss: 15.823877 \n",
      "Step: 1604 loss: 15.802414 \n",
      "Step: 1605 loss: 15.808397 \n",
      "Step: 1606 loss: 15.771818 \n",
      "Step: 1607 loss: 15.739950 \n",
      "Step: 1608 loss: 15.764064 \n",
      "Step: 1609 loss: 15.715502 \n",
      "Accuracy:  0.8780563186813187\n",
      "Step: 1610 loss: 15.815689 \n",
      "Step: 1611 loss: 15.766147 \n",
      "Step: 1612 loss: 15.793020 \n",
      "Step: 1613 loss: 15.814546 \n",
      "Step: 1614 loss: 15.831634 \n",
      "Step: 1615 loss: 15.864025 \n",
      "Step: 1616 loss: 16.797335 \n",
      "Step: 1617 loss: 23.976891 \n",
      "Step: 1618 loss: 17.654643 \n",
      "Step: 1619 loss: 22.987760 \n",
      "Accuracy:  0.8635645604395604\n",
      "Step: 1620 loss: 20.578452 \n",
      "Step: 1621 loss: 18.497387 \n",
      "Step: 1622 loss: 18.461437 \n",
      "Step: 1623 loss: 18.046570 \n",
      "Step: 1624 loss: 17.963798 \n",
      "Step: 1625 loss: 18.132883 \n",
      "Step: 1626 loss: 18.218595 \n",
      "Step: 1627 loss: 17.779670 \n",
      "Step: 1628 loss: 17.777525 \n",
      "Step: 1629 loss: 17.586885 \n",
      "Accuracy:  0.871875\n",
      "Step: 1630 loss: 17.467271 \n",
      "Step: 1631 loss: 17.468995 \n",
      "Step: 1632 loss: 17.567042 \n",
      "Step: 1633 loss: 17.666769 \n",
      "Step: 1634 loss: 17.474728 \n",
      "Step: 1635 loss: 17.442106 \n",
      "Step: 1636 loss: 17.437539 \n",
      "Step: 1637 loss: 17.432341 \n",
      "Step: 1638 loss: 17.609343 \n",
      "Step: 1639 loss: 17.665103 \n",
      "Accuracy:  0.8698489010989011\n",
      "Step: 1640 loss: 17.387672 \n",
      "Step: 1641 loss: 17.241773 \n",
      "Step: 1642 loss: 17.929751 \n",
      "Step: 1643 loss: 18.192606 \n",
      "Step: 1644 loss: 17.782562 \n",
      "Step: 1645 loss: 17.083490 \n",
      "Step: 1646 loss: 16.480134 \n",
      "Step: 1647 loss: 17.914854 \n",
      "Step: 1648 loss: 17.325004 \n",
      "Step: 1649 loss: 17.098434 \n",
      "Accuracy:  0.8702953296703296\n",
      "Step: 1650 loss: 17.095090 \n",
      "Step: 1651 loss: 17.112524 \n",
      "Step: 1652 loss: 16.665283 \n",
      "Step: 1653 loss: 16.750789 \n",
      "Step: 1654 loss: 16.338678 \n",
      "Step: 1655 loss: 16.144518 \n",
      "Step: 1656 loss: 16.070761 \n",
      "Step: 1657 loss: 16.026869 \n",
      "Step: 1658 loss: 16.086844 \n",
      "Step: 1659 loss: 16.133675 \n",
      "Accuracy:  0.8754464285714286\n",
      "Step: 1660 loss: 16.284365 \n",
      "Step: 1661 loss: 16.066060 \n",
      "Step: 1662 loss: 15.937873 \n",
      "Step: 1663 loss: 15.887739 \n",
      "Step: 1664 loss: 15.948148 \n",
      "Step: 1665 loss: 16.391829 \n",
      "Step: 1666 loss: 17.193262 \n",
      "Step: 1667 loss: 16.555243 \n",
      "Step: 1668 loss: 15.788949 \n",
      "Step: 1669 loss: 16.024132 \n",
      "Accuracy:  0.8739354395604395\n",
      "Step: 1670 loss: 16.136632 \n",
      "Step: 1671 loss: 15.805806 \n",
      "Step: 1672 loss: 15.633844 \n",
      "Step: 1673 loss: 15.718284 \n",
      "Step: 1674 loss: 15.658570 \n",
      "Step: 1675 loss: 15.803388 \n",
      "Step: 1676 loss: 15.945706 \n",
      "Step: 1677 loss: 16.004738 \n",
      "Step: 1678 loss: 15.760059 \n",
      "Step: 1679 loss: 15.627732 \n",
      "Accuracy:  0.8782623626373627\n",
      "Step: 1680 loss: 15.729963 \n",
      "Step: 1681 loss: 15.790314 \n",
      "Step: 1682 loss: 16.032435 \n",
      "Step: 1683 loss: 15.754976 \n",
      "Step: 1684 loss: 15.634172 \n",
      "Step: 1685 loss: 15.621052 \n",
      "Step: 1686 loss: 15.649605 \n",
      "Step: 1687 loss: 15.623557 \n",
      "Step: 1688 loss: 15.532337 \n",
      "Step: 1689 loss: 15.731900 \n",
      "Accuracy:  0.879739010989011\n",
      "Step: 1690 loss: 15.724360 \n",
      "Step: 1691 loss: 15.751137 \n",
      "Step: 1692 loss: 15.980585 \n",
      "Step: 1693 loss: 15.947365 \n",
      "Step: 1694 loss: 15.833452 \n",
      "Step: 1695 loss: 15.804533 \n",
      "Step: 1696 loss: 15.777207 \n",
      "Step: 1697 loss: 16.437106 \n",
      "Step: 1698 loss: 16.120816 \n",
      "Step: 1699 loss: 15.769580 \n",
      "Accuracy:  0.879739010989011\n",
      "Step: 1700 loss: 15.649875 \n",
      "Step: 1701 loss: 15.608094 \n",
      "Step: 1702 loss: 15.513063 \n",
      "Step: 1703 loss: 15.592744 \n",
      "Step: 1704 loss: 15.714178 \n",
      "Step: 1705 loss: 15.469684 \n",
      "Step: 1706 loss: 15.666195 \n",
      "Step: 1707 loss: 15.669963 \n",
      "Step: 1708 loss: 15.862300 \n",
      "Step: 1709 loss: 15.461798 \n",
      "Accuracy:  0.8813873626373626\n",
      "Step: 1710 loss: 15.517876 \n",
      "Step: 1711 loss: 15.497434 \n",
      "Step: 1712 loss: 15.475395 \n",
      "Step: 1713 loss: 15.663056 \n",
      "Step: 1714 loss: 15.483488 \n",
      "Step: 1715 loss: 16.620035 \n",
      "Step: 1716 loss: 17.147554 \n",
      "Step: 1717 loss: 18.811075 \n",
      "Step: 1718 loss: 18.206411 \n",
      "Step: 1719 loss: 17.660759 \n",
      "Accuracy:  0.871978021978022\n",
      "Step: 1720 loss: 17.031544 \n",
      "Step: 1721 loss: 16.771081 \n",
      "Step: 1722 loss: 16.804355 \n",
      "Step: 1723 loss: 17.605491 \n",
      "Step: 1724 loss: 16.889281 \n",
      "Step: 1725 loss: 17.323321 \n",
      "Step: 1726 loss: 16.703800 \n",
      "Step: 1727 loss: 17.104206 \n",
      "Step: 1728 loss: 16.846924 \n",
      "Step: 1729 loss: 16.599532 \n",
      "Accuracy:  0.8771291208791209\n",
      "Step: 1730 loss: 16.464479 \n",
      "Step: 1731 loss: 16.329984 \n",
      "Step: 1732 loss: 16.286190 \n",
      "Step: 1733 loss: 15.816363 \n",
      "Step: 1734 loss: 15.887748 \n",
      "Step: 1735 loss: 15.901384 \n",
      "Step: 1736 loss: 16.694942 \n",
      "Step: 1737 loss: 17.823160 \n",
      "Step: 1738 loss: 18.328617 \n",
      "Step: 1739 loss: 16.909369 \n",
      "Accuracy:  0.8709821428571428\n",
      "Step: 1740 loss: 16.968757 \n",
      "Step: 1741 loss: 17.001384 \n",
      "Step: 1742 loss: 17.078217 \n",
      "Step: 1743 loss: 16.797620 \n",
      "Step: 1744 loss: 17.780135 \n",
      "Step: 1745 loss: 17.315346 \n",
      "Step: 1746 loss: 16.826778 \n",
      "Step: 1747 loss: 17.436899 \n",
      "Step: 1748 loss: 16.700198 \n",
      "Step: 1749 loss: 16.505549 \n",
      "Accuracy:  0.8802197802197802\n",
      "Step: 1750 loss: 16.319689 \n",
      "Step: 1751 loss: 16.273918 \n",
      "Step: 1752 loss: 16.390985 \n",
      "Step: 1753 loss: 16.292490 \n",
      "Step: 1754 loss: 16.126633 \n",
      "Step: 1755 loss: 16.222040 \n",
      "Step: 1756 loss: 16.111734 \n",
      "Step: 1757 loss: 16.225549 \n",
      "Step: 1758 loss: 16.203227 \n",
      "Step: 1759 loss: 16.168946 \n",
      "Accuracy:  0.879842032967033\n",
      "Step: 1760 loss: 16.514806 \n",
      "Step: 1761 loss: 21.511025 \n",
      "Step: 1762 loss: 16.542144 \n",
      "Step: 1763 loss: 16.318339 \n",
      "Step: 1764 loss: 16.208981 \n",
      "Step: 1765 loss: 16.241968 \n",
      "Step: 1766 loss: 16.105533 \n",
      "Step: 1767 loss: 15.882996 \n",
      "Step: 1768 loss: 15.777635 \n",
      "Step: 1769 loss: 15.888173 \n",
      "Accuracy:  0.8802541208791209\n",
      "Step: 1770 loss: 15.617680 \n",
      "Step: 1771 loss: 15.810144 \n",
      "Step: 1772 loss: 15.812680 \n",
      "Step: 1773 loss: 15.868434 \n",
      "Step: 1774 loss: 15.568214 \n",
      "Step: 1775 loss: 15.452400 \n",
      "Step: 1776 loss: 15.535134 \n",
      "Step: 1777 loss: 15.528877 \n",
      "Step: 1778 loss: 15.619407 \n",
      "Step: 1779 loss: 15.441563 \n",
      "Accuracy:  0.8799450549450549\n",
      "Step: 1780 loss: 15.529896 \n",
      "Step: 1781 loss: 15.543741 \n",
      "Step: 1782 loss: 15.639925 \n",
      "Step: 1783 loss: 15.457675 \n",
      "Step: 1784 loss: 15.475261 \n",
      "Step: 1785 loss: 15.446090 \n",
      "Step: 1786 loss: 15.510230 \n",
      "Step: 1787 loss: 15.560545 \n",
      "Step: 1788 loss: 15.691279 \n",
      "Step: 1789 loss: 15.477193 \n",
      "Accuracy:  0.8808722527472528\n",
      "Step: 1790 loss: 15.471438 \n",
      "Step: 1791 loss: 15.408381 \n",
      "Step: 1792 loss: 15.444690 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1793 loss: 15.584761 \n",
      "Step: 1794 loss: 15.641959 \n",
      "Step: 1795 loss: 15.454220 \n",
      "Step: 1796 loss: 15.461348 \n",
      "Step: 1797 loss: 15.386760 \n",
      "Step: 1798 loss: 15.501525 \n",
      "Step: 1799 loss: 15.802795 \n",
      "Accuracy:  0.8756868131868132\n",
      "Step: 1800 loss: 15.851270 \n",
      "Step: 1801 loss: 15.962657 \n",
      "Step: 1802 loss: 18.335429 \n",
      "Step: 1803 loss: 17.272947 \n",
      "Step: 1804 loss: 16.689483 \n",
      "Step: 1805 loss: 16.589290 \n",
      "Step: 1806 loss: 16.375030 \n",
      "Step: 1807 loss: 16.462237 \n",
      "Step: 1808 loss: 17.925519 \n",
      "Step: 1809 loss: 16.896526 \n",
      "Accuracy:  0.8741414835164835\n",
      "Step: 1810 loss: 16.570686 \n",
      "Step: 1811 loss: 16.400326 \n",
      "Step: 1812 loss: 16.275594 \n",
      "Step: 1813 loss: 16.223461 \n",
      "Step: 1814 loss: 16.258367 \n",
      "Step: 1815 loss: 16.187939 \n",
      "Step: 1816 loss: 16.272567 \n",
      "Step: 1817 loss: 16.542487 \n",
      "Step: 1818 loss: 16.560170 \n",
      "Step: 1819 loss: 16.336544 \n",
      "Accuracy:  0.8784684065934066\n",
      "Step: 1820 loss: 16.226267 \n",
      "Step: 1821 loss: 16.220530 \n",
      "Step: 1822 loss: 16.141827 \n",
      "Step: 1823 loss: 16.267190 \n",
      "Step: 1824 loss: 16.212421 \n",
      "Step: 1825 loss: 16.360982 \n",
      "Step: 1826 loss: 16.196376 \n",
      "Step: 1827 loss: 16.206704 \n",
      "Step: 1828 loss: 17.035337 \n",
      "Step: 1829 loss: 16.758069 \n",
      "Accuracy:  0.8758241758241758\n",
      "Step: 1830 loss: 16.372681 \n",
      "Step: 1831 loss: 16.355420 \n",
      "Step: 1832 loss: 16.165966 \n",
      "Step: 1833 loss: 16.209279 \n",
      "Step: 1834 loss: 16.171774 \n",
      "Step: 1835 loss: 16.100865 \n",
      "Step: 1836 loss: 16.171939 \n",
      "Step: 1837 loss: 16.132563 \n",
      "Step: 1838 loss: 16.277716 \n",
      "Step: 1839 loss: 16.425490 \n",
      "Accuracy:  0.8737293956043956\n",
      "Step: 1840 loss: 16.535491 \n",
      "Step: 1841 loss: 16.258660 \n",
      "Step: 1842 loss: 16.207351 \n",
      "Step: 1843 loss: 16.153697 \n",
      "Step: 1844 loss: 16.103648 \n",
      "Step: 1845 loss: 16.233656 \n",
      "Step: 1846 loss: 16.454044 \n",
      "Step: 1847 loss: 16.598357 \n",
      "Step: 1848 loss: 16.304790 \n",
      "Step: 1849 loss: 16.147203 \n",
      "Accuracy:  0.8777815934065935\n",
      "Step: 1850 loss: 16.213359 \n",
      "Step: 1851 loss: 16.264318 \n",
      "Step: 1852 loss: 16.388663 \n",
      "Step: 1853 loss: 16.199844 \n",
      "Step: 1854 loss: 16.570748 \n",
      "Step: 1855 loss: 16.351449 \n",
      "Step: 1856 loss: 16.356736 \n",
      "Step: 1857 loss: 16.336616 \n",
      "Step: 1858 loss: 16.422209 \n",
      "Step: 1859 loss: 16.235541 \n",
      "Accuracy:  0.8779189560439561\n",
      "Step: 1860 loss: 16.195651 \n",
      "Step: 1861 loss: 16.137073 \n",
      "Step: 1862 loss: 16.208624 \n",
      "Step: 1863 loss: 19.118265 \n",
      "Step: 1864 loss: 74.500616 \n",
      "Step: 1865 loss: 110.450825 \n",
      "Step: 1866 loss: 19.975499 \n",
      "Step: 1867 loss: 18.419833 \n",
      "Step: 1868 loss: 17.880892 \n",
      "Step: 1869 loss: 17.658341 \n",
      "Accuracy:  0.8718063186813186\n",
      "Step: 1870 loss: 17.364848 \n",
      "Step: 1871 loss: 17.820285 \n",
      "Step: 1872 loss: 17.080726 \n",
      "Step: 1873 loss: 16.810599 \n",
      "Step: 1874 loss: 16.745362 \n",
      "Step: 1875 loss: 16.653093 \n",
      "Step: 1876 loss: 16.641990 \n",
      "Step: 1877 loss: 16.562796 \n",
      "Step: 1878 loss: 16.579067 \n",
      "Step: 1879 loss: 16.560458 \n",
      "Accuracy:  0.8768543956043956\n",
      "Step: 1880 loss: 16.551948 \n",
      "Step: 1881 loss: 16.565374 \n",
      "Step: 1882 loss: 16.534473 \n",
      "Step: 1883 loss: 16.489285 \n",
      "Step: 1884 loss: 16.427982 \n",
      "Step: 1885 loss: 16.434280 \n",
      "Step: 1886 loss: 16.381859 \n",
      "Step: 1887 loss: 16.408230 \n",
      "Step: 1888 loss: 16.365953 \n",
      "Step: 1889 loss: 16.386599 \n",
      "Accuracy:  0.878331043956044\n",
      "Step: 1890 loss: 16.332194 \n",
      "Step: 1891 loss: 16.381039 \n",
      "Step: 1892 loss: 16.384293 \n",
      "Step: 1893 loss: 16.391832 \n",
      "Step: 1894 loss: 16.319811 \n",
      "Step: 1895 loss: 16.350194 \n",
      "Step: 1896 loss: 16.356009 \n",
      "Step: 1897 loss: 16.342175 \n",
      "Step: 1898 loss: 16.236845 \n",
      "Step: 1899 loss: 16.284389 \n",
      "Accuracy:  0.8791895604395604\n",
      "Step: 1900 loss: 16.227205 \n",
      "Step: 1901 loss: 16.224972 \n",
      "Step: 1902 loss: 16.232765 \n",
      "Step: 1903 loss: 16.199463 \n",
      "Step: 1904 loss: 16.226536 \n",
      "Step: 1905 loss: 16.200881 \n",
      "Step: 1906 loss: 16.179679 \n",
      "Step: 1907 loss: 16.200658 \n",
      "Step: 1908 loss: 16.172039 \n",
      "Step: 1909 loss: 16.169613 \n",
      "Accuracy:  0.8785714285714286\n",
      "Step: 1910 loss: 16.173306 \n",
      "Step: 1911 loss: 16.168189 \n",
      "Step: 1912 loss: 16.148663 \n",
      "Step: 1913 loss: 16.203262 \n",
      "Step: 1914 loss: 16.107676 \n",
      "Step: 1915 loss: 16.224520 \n",
      "Step: 1916 loss: 16.079467 \n",
      "Step: 1917 loss: 16.293942 \n",
      "Step: 1918 loss: 16.241242 \n",
      "Step: 1919 loss: 16.452340 \n",
      "Accuracy:  0.8788461538461538\n",
      "Step: 1920 loss: 16.157348 \n",
      "Step: 1921 loss: 16.256140 \n",
      "Step: 1922 loss: 16.128754 \n",
      "Step: 1923 loss: 16.117025 \n",
      "Step: 1924 loss: 16.074133 \n",
      "Step: 1925 loss: 16.117928 \n",
      "Step: 1926 loss: 16.053405 \n",
      "Step: 1927 loss: 16.173771 \n",
      "Step: 1928 loss: 16.126486 \n",
      "Step: 1929 loss: 16.183220 \n",
      "Accuracy:  0.8785027472527472\n",
      "Step: 1930 loss: 15.810291 \n",
      "Step: 1931 loss: 16.039675 \n",
      "Step: 1932 loss: 16.417367 \n",
      "Step: 1933 loss: 19.338118 \n",
      "Step: 1934 loss: 19.011349 \n",
      "Step: 1935 loss: 17.174217 \n",
      "Step: 1936 loss: 17.067575 \n",
      "Step: 1937 loss: 17.008531 \n",
      "Step: 1938 loss: 17.049165 \n",
      "Step: 1939 loss: 16.981960 \n",
      "Accuracy:  0.8726991758241758\n",
      "Step: 1940 loss: 17.075742 \n",
      "Step: 1941 loss: 16.990504 \n",
      "Step: 1942 loss: 17.072746 \n",
      "Step: 1943 loss: 17.144181 \n",
      "Step: 1944 loss: 17.255508 \n",
      "Step: 1945 loss: 17.046382 \n",
      "Step: 1946 loss: 17.042134 \n",
      "Step: 1947 loss: 16.999400 \n",
      "Step: 1948 loss: 16.420318 \n",
      "Step: 1949 loss: 16.214014 \n",
      "Accuracy:  0.8759271978021979\n",
      "Step: 1950 loss: 16.399956 \n",
      "Step: 1951 loss: 16.658862 \n",
      "Step: 1952 loss: 16.757869 \n",
      "Step: 1953 loss: 16.368897 \n",
      "Step: 1954 loss: 16.576509 \n",
      "Step: 1955 loss: 15.502109 \n",
      "Step: 1956 loss: 24.382759 \n",
      "Step: 1957 loss: 16.653751 \n",
      "Step: 1958 loss: 16.240050 \n",
      "Step: 1959 loss: 15.926311 \n",
      "Accuracy:  0.8777129120879121\n",
      "Step: 1960 loss: 15.766336 \n",
      "Step: 1961 loss: 15.666900 \n",
      "Step: 1962 loss: 15.612101 \n",
      "Step: 1963 loss: 15.604330 \n",
      "Step: 1964 loss: 15.601749 \n",
      "Step: 1965 loss: 15.591919 \n",
      "Step: 1966 loss: 15.564284 \n",
      "Step: 1967 loss: 15.562406 \n",
      "Step: 1968 loss: 15.561088 \n",
      "Step: 1969 loss: 15.560771 \n",
      "Accuracy:  0.8788804945054945\n",
      "Step: 1970 loss: 15.566354 \n",
      "Step: 1971 loss: 15.551624 \n",
      "Step: 1972 loss: 15.564979 \n",
      "Step: 1973 loss: 15.580563 \n",
      "Step: 1974 loss: 15.542739 \n",
      "Step: 1975 loss: 15.535664 \n",
      "Step: 1976 loss: 15.578421 \n",
      "Step: 1977 loss: 15.535729 \n",
      "Step: 1978 loss: 15.605431 \n",
      "Step: 1979 loss: 15.535978 \n",
      "Accuracy:  0.8784340659340659\n",
      "Step: 1980 loss: 15.633560 \n",
      "Step: 1981 loss: 15.599662 \n",
      "Step: 1982 loss: 15.747822 \n",
      "Step: 1983 loss: 16.173014 \n",
      "Step: 1984 loss: 16.203359 \n",
      "Step: 1985 loss: 16.397603 \n",
      "Step: 1986 loss: 15.902553 \n",
      "Step: 1987 loss: 15.957148 \n",
      "Step: 1988 loss: 15.892674 \n",
      "Step: 1989 loss: 15.746521 \n",
      "Accuracy:  0.8785714285714286\n",
      "Step: 1990 loss: 15.697171 \n",
      "Step: 1991 loss: 15.460881 \n",
      "Step: 1992 loss: 15.551144 \n",
      "Step: 1993 loss: 15.428237 \n",
      "Step: 1994 loss: 15.566323 \n",
      "Step: 1995 loss: 15.439027 \n",
      "Step: 1996 loss: 15.594353 \n",
      "Step: 1997 loss: 16.047227 \n",
      "Step: 1998 loss: 16.106257 \n",
      "Step: 1999 loss: 15.669639 \n",
      "Accuracy:  0.8806318681318681\n"
     ]
    }
   ],
   "source": [
    "step=0\n",
    "saver=tf.train.Saver(max_to_keep=1)\n",
    "try:\n",
    "    checkpoint = tf.train.latest_checkpoint('checkpoint')\n",
    "    saver.restore(sess, checkpoint)\n",
    "    step=int(checkpoint.split('-')[-1])+1\n",
    "    print('Load complete at step: ',str(step))\n",
    "except Exception:\n",
    "    print('Load model fail')\n",
    "    step=0\n",
    "for epoch in range(step,2000):\n",
    "    L=0\n",
    "    for iters in range(data_train.shape[0]//batch_size):\n",
    "        _,l=sess.run([optimizer,tf_loss],feed_dict={train_inputs:data_train[iters*batch_size:(iters+1)*batch_size],train_labels:a2.label[iters*batch_size:(iters+1)*batch_size]})\n",
    "        L+=l\n",
    "    print('Step: {:d} loss: {:f} '.format(epoch,L))\n",
    "    if (epoch+1)%10==0:\n",
    "        saver.save(sess,'checkpoint/model',global_step=epoch)\n",
    "        count=0\n",
    "        for iters in range(data_test.shape[0]//batch_size):\n",
    "            predict=sess.run(tf_predictions,feed_dict={train_inputs:data_test[iters*batch_size:(iters+1)*batch_size]})\n",
    "            predict=np.argmax(predict,axis=1)\n",
    "            count+=sum(predict==a1.label[iters*batch_size:(iters+1)*batch_size])\n",
    "        print('Accuracy: ',str(count/len(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for iters in range(data_test.shape[0]//batch_size):\n",
    "    predict=sess.run(tf_predictions,feed_dict={train_inputs:np.reshape(data_test[iters*batch_size:(iters+1)*batch_size],[batch_size,-1])})\n",
    "    predict=np.argmax(predict,axis=1)\n",
    "    count+=sum(predict==a1.label[iters*batch_size:(iters+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93     50400\n",
      "           1       1.00      0.57      0.72     17545\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     67945\n",
      "   macro avg       0.93      0.78      0.83     67945\n",
      "weighted avg       0.90      0.89      0.88     67945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93     21600\n",
      "           1       0.99      0.56      0.71      7520\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     29120\n",
      "   macro avg       0.93      0.78      0.82     29120\n",
      "weighted avg       0.90      0.88      0.87     29120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict=sess.run(tf_predictions,feed_dict={train_inputs:data_train})\n",
    "predict=np.argmax(predict,axis=1)\n",
    "print(classification_report(a2.label, predict))\n",
    "predict=sess.run(tf_predictions,feed_dict={train_inputs:data_test})\n",
    "predict=np.argmax(predict,axis=1)\n",
    "print(classification_report(a1.label, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
