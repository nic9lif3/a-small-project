{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=pd.read_csv('testing.csv')\n",
    "a2=pd.read_csv('trainning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept1=[]\n",
    "accept2=[]\n",
    "\n",
    "for key in a1.keys():\n",
    "    if len(np.unique(a1[key]))>1:\n",
    "        accept1.append(key)\n",
    "for key in a2.keys():\n",
    "    if len(np.unique(a2[key]))>1:\n",
    "        accept2.append(key) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normarlize(length):\n",
    "    return (length-length.mean())/length.std(),length.mean(),length.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 116.55it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 296.57it/s]\n"
     ]
    }
   ],
   "source": [
    "b1=list()\n",
    "b2=list()\n",
    "des=dict()\n",
    "accept2=np.delete(accept2,np.argwhere(accept2=='label'))\n",
    "for key in tqdm(sorted(accept2)):\n",
    "    if a2[key].dtype=='int64' or a2[key].dtype=='float64':\n",
    "        nm,mean,std=normarlize(a2[key])\n",
    "        nm=np.expand_dims(nm.tolist(),0).T\n",
    "        des[key]=[mean,std]\n",
    "    elif a2[key].dtype=='O':\n",
    "        uni=a2[key].unique()\n",
    "        l=len(uni)\n",
    "        if '-1' in uni:\n",
    "            l-=1\n",
    "        nm=np.zeros((len(a2[key]),l))\n",
    "        uni=np.delete(uni,np.argwhere(uni=='-1'))\n",
    "        uni=dict(zip(uni,list(range(len(uni)))))\n",
    "        des[key]=uni\n",
    "        for i,n in enumerate(a2[key]):\n",
    "            try:\n",
    "                nm[i,uni[n]]=1\n",
    "            except:\n",
    "                continue  \n",
    "    else:\n",
    "        continue\n",
    "    b2.append(nm)    \n",
    "    \n",
    "for key in tqdm(sorted(accept2)):\n",
    "    if a1[key].dtype=='int64' or a1[key].dtype=='float64':\n",
    "        nm=(a1[key]-des[key][0])/des[key][1]\n",
    "        nm=np.expand_dims(nm.tolist(),0).T\n",
    "    elif a1[key].dtype=='O':\n",
    "        nm=np.zeros((len(a1[key]),len(des[key].keys())))\n",
    "        for i,n in enumerate(a1[key]):\n",
    "            try:\n",
    "                nm[i,des[key][n]]=1\n",
    "            except:\n",
    "                continue  \n",
    "    else:\n",
    "        continue\n",
    "    b1.append(nm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=np.concatenate(b2,axis=1)\n",
    "data_test=np.concatenate(b1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclassifier =SVC(kernel='linear',verbose=True,C=5)\n",
    "svclassifier.fit(data_train,a2.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91     50400\n",
      "           1       1.00      0.43      0.60     17545\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     67945\n",
      "   macro avg       0.92      0.72      0.76     67945\n",
      "weighted avg       0.88      0.85      0.83     67945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91     21600\n",
      "           1       1.00      0.43      0.60      7520\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     29120\n",
      "   macro avg       0.92      0.71      0.75     29120\n",
      "weighted avg       0.88      0.85      0.83     29120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred=svclassifier.predict(data_train)\n",
    "print(classification_report(a2.label, pred))\n",
    "pred=svclassifier.predict(data_test)\n",
    "print(classification_report(a1.label, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model fail\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "layer_ids = ['hidden1','hidden2','out']\n",
    "layer_sizes = [data_train.shape[1], 200,150, 2]\n",
    "train_inputs = tf.placeholder(tf.float32, shape=[None, layer_sizes[0]], name='train_inputs')\n",
    "train_labels = tf.placeholder(tf.int32, shape=[None], name='train_labels')\n",
    "for idx, lid in enumerate(layer_ids):\n",
    "    with tf.variable_scope(lid):\n",
    "        w = tf.get_variable('weights',shape=[layer_sizes[idx], layer_sizes[idx+1]],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "        b = tf.get_variable('bias',shape= [layer_sizes[idx+1]],\n",
    "                            initializer=tf.random_uniform_initializer(-0.1,0.1))\n",
    "h = train_inputs\n",
    "for lid in layer_ids:\n",
    "    with tf.variable_scope(lid,reuse=True):\n",
    "        w, b = tf.get_variable('weights'), tf.get_variable('bias')\n",
    "        if lid != 'out':\n",
    "            h = tf.nn.relu(tf.matmul(h,w)+b,name=lid+'_output')\n",
    "        else:\n",
    "            h = tf.nn.xw_plus_b(h,w,b,name=lid+'_output')\n",
    "tf_predictions = tf.nn.softmax(h, name='predictions')\n",
    "tf_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(train_labels,depth=2), logits=h),name='loss')\n",
    "tf_learning_rate = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001,).minimize(tf_loss)\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step=0\n",
    "saver=tf.train.Saver(max_to_keep=1)\n",
    "try:\n",
    "    checkpoint = tf.train.latest_checkpoint('checkpoint')\n",
    "    saver.restore(sess, checkpoint)\n",
    "    step=int(checkpoint.split('-')[-1])+1\n",
    "    print('Load complete at step: ',str(step))\n",
    "except Exception:\n",
    "    print('Load model fail')\n",
    "    step=0\n",
    "for epoch in range(step,2000):\n",
    "    L=0\n",
    "    for iters in range(data_train.shape[0]//batch_size):\n",
    "        _,l=sess.run([optimizer,tf_loss],feed_dict={train_inputs:data_train[iters*batch_size:(iters+1)*batch_size],train_labels:a2.label[iters*batch_size:(iters+1)*batch_size]})\n",
    "        L+=l\n",
    "    print('Step: {:d} loss: {:f} '.format(epoch,L))\n",
    "    if (epoch+1)%10==0:\n",
    "        saver.save(sess,'checkpoint/model',global_step=epoch)\n",
    "        count=0\n",
    "        for iters in range(data_test.shape[0]//batch_size):\n",
    "            predict=sess.run(tf_predictions,feed_dict={train_inputs:data_test[iters*batch_size:(iters+1)*batch_size]})\n",
    "            predict=np.argmax(predict,axis=1)\n",
    "            count+=sum(predict==a1.label[iters*batch_size:(iters+1)*batch_size])\n",
    "        print('Accuracy: ',str(count/len(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for iters in range(data_test.shape[0]//batch_size):\n",
    "    predict=sess.run(tf_predictions,feed_dict={train_inputs:np.reshape(data_test[iters*batch_size:(iters+1)*batch_size],[batch_size,-1])})\n",
    "    predict=np.argmax(predict,axis=1)\n",
    "    count+=sum(predict==a1.label[iters*batch_size:(iters+1)*batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93     50400\n",
      "           1       1.00      0.57      0.72     17545\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     67945\n",
      "   macro avg       0.93      0.78      0.83     67945\n",
      "weighted avg       0.90      0.89      0.88     67945\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93     21600\n",
      "           1       0.99      0.56      0.71      7520\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     29120\n",
      "   macro avg       0.93      0.78      0.82     29120\n",
      "weighted avg       0.90      0.88      0.87     29120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict=sess.run(tf_predictions,feed_dict={train_inputs:data_train})\n",
    "predict=np.argmax(predict,axis=1)\n",
    "print(classification_report(a2.label, predict))\n",
    "predict=sess.run(tf_predictions,feed_dict={train_inputs:data_test})\n",
    "predict=np.argmax(predict,axis=1)\n",
    "print(classification_report(a1.label, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
